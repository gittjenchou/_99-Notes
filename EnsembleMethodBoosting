

ENSEMBLE METHOD
The idea is to take multiple classification and combine it to get the better classification.
One thing to do that is by averaging multiple classifications or taking the most vote across all classifications.
Ensemble method includes:
-- bagging
-- random forest
-- boosting (most popular: AdaBoost)
Boosting seems to base the idea of correcting weak tree. It will select the weak tree in which misclassification is high and fix it. And so on giving higher weight to weak tree and fix it.


BOOSTING AND ADABOOST
Boosting is an ensemble technique that attempts to create a strong classifier from a number of weak classifiers.
The individual learners can be weak, but as long as the performance of each one is slightly better than random guessing.
This is done by building a model from the training data, then creating a second model that attempts to correct the errors from the first model.
Models are added until the training set is predicted perfectly or a maximum number of models are added.
AdaBoost is sensitive to noisy data and outliers.
This can happen when applying weight to outlier.
AdaBoost was the first really successful boosting algorithm developed for binary classification.
It is the best starting point for understanding boosting.
Modern boosting methods build on AdaBoost, most notably stochastic gradient boosting machines.
It is used for classification rather than regression.
For one weak model, each records will have a weight. Error rate and stage value is calculated.
Error rate is #incorrect prediction out of total records. Stage value is ln[(1-error rate)/error rate]
Weight later is updated using initial weight * exp(stage value * perror).
Perror is 0 for correct prediction or 1 for incorrect prediction.
The result would be weight is higher for misclassified records.
When all weak models are available, predictions are made by calculating the weighted average of the weak classifiers.
The predicted values are weighted by each weak learners stage value.
The prediction for the ensemble model is taken as the sum of the weighted predictions.
If the sum is positive, then the first class is predicted, if negative the second class is predicted (in case of binary).
??? find more reference
