MAXIMUM LIKELIHOOD ESTIMATION (MLE)
_______________________________________

Is a parameter estimation method
Idea: finding parameter that maximizes the likelihood (function) observing our data given the parameter

MLE can be seen as a special case of the maximum a posteriori estimation (MAP) that assumes a uniform prior distribution of the parameters, OR as a variant of the MAP that ignores the prior and which therefore is unregularized.

This is to say that: MLE coincides with the most probable Bayesian estimator given a uniform prior distribution on the parameters.

The way it works: Identify likelihood function :function of observing data given the parameter, which will be related to Bayes theorem (conditional density function). Usually it will be a lot easier to work with log likelihood since log function is monotonic increasing function.

Finding the argmax of likelihood function can be done with derivative of this log likelihood function.

https://en.wikipedia.org/wiki/Maximum_likelihood_estimation



MAXIMUM A POSTERIORI ESTIMATION (MAP)
_______________________________________

Is a estimate that equal to modes of posterior distribution.
Is a parameter estimate that maximizes likelihood (function) observing our data given certain parameter, that is generated by a distribution function. This is representation of likelihood function and prior distribution which resulted to posterior distribution.

That's why MLE is MAP given that the parameter is uniform (fixed).

What has to be noted is the fact that MAP is a point estimate, whereas Bayesian methods are characterized by the use of distributions to summarize data and draw inferences: thus, Bayesian methods tend to report the posterior mean or median instead, together with credible intervals.

https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation



EXPECTATION MAXIMIZATION ALGORITHM
_______________________________________

Iterative method to find parameter estimate in MAP or MLE.

When we define likelihood function that maximizes observing our data given a parameter, the "event" of observing our data is not necesssarily simple, sometimes it's a sequence of events (imagine hidden Markov model).

https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm



