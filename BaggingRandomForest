

BOOSTRAP AGGREGATION (BAGGING)
Say there is sample data N, we perform random sampling with replacement size M 100x.
For each bootstrap sample, we perform fitting with LOESS, resulted to, tendency to overfit, 100 models.
From all models, we take the average which would be less overfit compared to model from bootstrap.


LOESS : LOCAL REGRESSION
Create subset of data with Nearest Neighbor
User to define bandwidth or smoothing parameter alpha: how much of the data (in the subset) is used to fit the local polynomial.
The smaller alpha is, the closer the regression function will conform (noise) from the data.
Using too small a value of the smoothing parameter is not desirable, since the regression function will eventually start to capture the random error in the data.
Useful values of the smoothing parameter typically lie in the range 0.25 to 0.5 for most LOESS applications.
Degree of polynomial is usually low: 1 or 2.
The polynomial is fitted using weighted least squares, giving : 
more weight to points near the point whose response is being estimated and 
less weight to points further away. 
Weight function is limited choices, e.g.: tricube weight function


RANDOM SUBSPACE METHOD 
Similar to bagging except that for features / attributes.
Feature are randomly sampled, WITHOUT replacement, for each learner. 
This causes individual learners to not over-focus on features that appear highly predictive/descriptive in the training set, 
but fail to be as predictive outside that set.
For this reason, random subspaces are an attractive choice for problems where the number of features is much larger than the number of training points.
When combined with "ordinary" bagging of decision trees, the resulting models are called random forests.
Says, #training points : N & #features : D.
Choose L to be #individual models.
For each individual model l, choose dl (dl < D) to be the #attributes for l. It is common to have only one value of dl for all the individual models.
Combine the outputs of the L individual models by majority voting or by combining the posterior probabilities.


BAGGING AND RANDOM FOREST
BOOTSTRAP METHOD
Let’s assume we have a sample of 100 values.
We’d like to get an estimate of the mean of the sample.
We know that our sample is small and that our mean has error in it. We can improve the estimate of our mean using the bootstrap procedure:
-- Create many (e.g. 1000) random sub-samples of our dataset with replacement (meaning we can select the same value multiple times).
-- Calculate the mean of each sub-sample.
-- Calculate the average of all of our collected means and use that as our estimated mean for the data.
BOOTSTRAP AGGREGATION (BAGGING)
Let’s assume we have a dataset of 1000 instances and we are using the CART algorithm.
Bagging of the CART algorithm would work as follows:
-- Create many (e.g. 100) random sub-samples of our dataset with replacement.
-- Train a CART model on each sample.
-- Given a new dataset, calculate the average prediction from each model.
For example, if we had 5 bagged decision trees that made the following class predictions for an input instance: blue,blue,red,blue and red.
We would take the most frequent class and predict blue .
When bagging with decision trees, we are less concerned about individual trees overfitting the training data.
For this reason and for efficiency, the individual decision trees are grown deep and the trees are not pruned.
These trees will have both high variance and low bias. 
These are important characteristics of sub-models when combining predictions using bagging.
RANDOM FOREST
Random Forests are an improvement over bagged decision trees. 
A problem with decision trees like CART is that they are greedy. They choose which variable to split on using a greedy algorithm that minimizes error.
As such, even with Bagging, the decision trees can have a lot of structural similarities and in turn result in high correlation in their predictions.
Combining predictions from multiple models in ensembles works better if the predictions from the sub-models are uncorrelated or at best weakly correlated.
This correlation is between the models.
Random forest changes the algorithm for the way that the sub-trees are learned so that the resulting predictions from all of the subtrees have less correlation. It is a simple tweak.
In CART, when selecting a split point, the learning algorithm is allowed to look through all variables and all variable values in order to select the most optimal split-point.
The random forest algorithm changes this procedure so that the learning algorithm is limited to a random sample of features of which to search.
The number of features that can be searched at each split point must be specified as a parameter to the algorithm.
You can try different values and tune it using cross validation.
For classification a good default is: m = √p.
For regression a good default is: m = p/3.
Where m is the number of randomly selected features that can be searched at a split point and p is the number of input variables.
ESTIMATED PERFORMANCE
For each bootstrap sample taken from the training data, there will be samples left behind that were not included.
These samples are called Out-Of-Bag samples or OOB. 
The performance of each model on its left out samples when averaged can provide an estimated accuracy of the bagged models.
This estimated performance is often called the OOB estimate.
These performance measures are a reliable estimate of test error and correlate well with cross validation estimates of error.
VARIABLE IMPORTANCE
As the Bagged decision trees are constructed, we can calculate how much the error function drops for a variable at each split point.
In regression problems this may be the drop in sum squared error and in classification this might be the Gini score.
These drops in error can be averaged across all decision trees and output to provide an estimate of the importance of each input variable.
The greater the drop when the variable was chosen, the greater the importance. 
